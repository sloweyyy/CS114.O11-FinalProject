{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.1.4-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "                                              0.0/10.6 MB ? eta -:--:--\n",
      "                                              0.0/10.6 MB ? eta -:--:--\n",
      "                                             0.0/10.6 MB 660.6 kB/s eta 0:00:17\n",
      "                                             0.1/10.6 MB 919.0 kB/s eta 0:00:12\n",
      "                                              0.2/10.6 MB 2.0 MB/s eta 0:00:06\n",
      "     -                                        0.4/10.6 MB 2.6 MB/s eta 0:00:04\n",
      "     --                                       0.6/10.6 MB 3.1 MB/s eta 0:00:04\n",
      "     ---                                      0.8/10.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ----                                     1.1/10.6 MB 4.3 MB/s eta 0:00:03\n",
      "     ----                                     1.3/10.6 MB 4.4 MB/s eta 0:00:03\n",
      "     -----                                    1.5/10.6 MB 4.5 MB/s eta 0:00:03\n",
      "     ------                                   1.8/10.6 MB 5.0 MB/s eta 0:00:02\n",
      "     -------                                  1.9/10.6 MB 4.9 MB/s eta 0:00:02\n",
      "     ---------                                2.6/10.6 MB 5.9 MB/s eta 0:00:02\n",
      "     -----------                              3.0/10.6 MB 6.4 MB/s eta 0:00:02\n",
      "     -------------                            3.5/10.6 MB 6.9 MB/s eta 0:00:02\n",
      "     --------------                           3.8/10.6 MB 7.0 MB/s eta 0:00:01\n",
      "     ----------------                         4.3/10.6 MB 7.4 MB/s eta 0:00:01\n",
      "     -----------------                        4.6/10.6 MB 7.5 MB/s eta 0:00:01\n",
      "     ------------------                       4.9/10.6 MB 7.7 MB/s eta 0:00:01\n",
      "     --------------------                     5.3/10.6 MB 7.7 MB/s eta 0:00:01\n",
      "     ---------------------                    5.7/10.6 MB 7.9 MB/s eta 0:00:01\n",
      "     -----------------------                  6.1/10.6 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------                 6.5/10.6 MB 8.3 MB/s eta 0:00:01\n",
      "     --------------------------               6.9/10.6 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------              7.3/10.6 MB 8.4 MB/s eta 0:00:01\n",
      "     -----------------------------            7.7/10.6 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------------------------           8.1/10.6 MB 8.7 MB/s eta 0:00:01\n",
      "     --------------------------------         8.6/10.6 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------        8.9/10.6 MB 8.9 MB/s eta 0:00:01\n",
      "     -----------------------------------      9.3/10.6 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------------     9.7/10.6 MB 9.0 MB/s eta 0:00:01\n",
      "     --------------------------------------   10.2/10.6 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.6/10.6 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.6/10.6 MB 9.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.1.4 pytz-2023.3.post1 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mtcnnNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
      "                                              0.0/2.3 MB ? eta -:--:--\n",
      "                                              0.0/2.3 MB ? eta -:--:--\n",
      "                                              0.0/2.3 MB 435.7 kB/s eta 0:00:06\n",
      "     -                                        0.1/2.3 MB 1.1 MB/s eta 0:00:02\n",
      "     ---                                      0.2/2.3 MB 1.5 MB/s eta 0:00:02\n",
      "     -------                                  0.4/2.3 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------                             0.7/2.3 MB 3.1 MB/s eta 0:00:01\n",
      "     -----------------                        1.0/2.3 MB 3.9 MB/s eta 0:00:01\n",
      "     -----------------------                  1.4/2.3 MB 4.8 MB/s eta 0:00:01\n",
      "     ----------------------------             1.6/2.3 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------     2.1/2.3 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.3/2.3 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.3/2.3 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting keras>=2.0.0 (from mtcnn)\n",
      "  Downloading keras-3.0.1-py3-none-any.whl (999 kB)\n",
      "                                              0.0/999.1 kB ? eta -:--:--\n",
      "     --------------                        399.4/999.1 kB 12.6 MB/s eta 0:00:01\n",
      "     ------------------                     481.3/999.1 kB 7.6 MB/s eta 0:00:01\n",
      "     -----------------------                624.6/999.1 kB 6.6 MB/s eta 0:00:01\n",
      "     --------------------------             706.6/999.1 kB 5.0 MB/s eta 0:00:01\n",
      "     -----------------------------          778.2/999.1 kB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------         809.0/999.1 kB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        819.2/999.1 kB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------        839.7/999.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------        839.7/999.1 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------       860.2/999.1 kB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------      870.4/999.1 kB 1.1 MB/s eta 0:00:01\n",
      "     -----------------------------------    921.6/999.1 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  993.3/999.1 kB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 999.1/999.1 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: opencv-python>=4.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mtcnn) (4.8.0.76)\n",
      "Requirement already satisfied: absl-py in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=2.0.0->mtcnn) (1.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=2.0.0->mtcnn) (1.25.2)\n",
      "Collecting rich (from keras>=2.0.0->mtcnn)\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "                                              0.0/240.6 kB ? eta -:--:--\n",
      "     ----                                    30.7/240.6 kB 1.3 MB/s eta 0:00:01\n",
      "     -----------------                      112.6/240.6 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 240.6/240.6 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting namex (from keras>=2.0.0->mtcnn)\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Collecting h5py (from keras>=2.0.0->mtcnn)\n",
      "  Using cached h5py-3.10.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Collecting dm-tree (from keras>=2.0.0->mtcnn)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl (101 kB)\n",
      "                                              0.0/101.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 101.3/101.3 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=2.0.0->mtcnn)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "                                              0.0/87.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 87.5/87.5 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=2.0.0->mtcnn) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->mtcnn)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, dm-tree, mdurl, h5py, markdown-it-py, rich, keras, mtcnn\n",
      "Successfully installed dm-tree-0.1.8 h5py-3.10.0 keras-3.0.1 markdown-it-py-3.0.0 mdurl-0.1.2 mtcnn-0.1.1 namex-0.0.7 rich-13.7.0\n"
     ]
    }
   ],
   "source": [
    "%pip install mtcnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = r'D:\\Final Project\\CS114.O11-FinalProject\\neutral\\input'\n",
    "output_directory = r'D:\\Final Project\\CS114.O11-FinalProject\\neutral\\output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Straightness face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "def is_face_straight(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    faces = detector.detect_faces(image)\n",
    "\n",
    "    if len(faces) == 1:\n",
    "        x, y, width, height = faces[0]['box']\n",
    "        face_center_x = x + width / 2\n",
    "        image_center_x = image.shape[1] / 2\n",
    "        face_offset_x = abs(face_center_x - image_center_x)\n",
    "\n",
    "        if face_offset_x <= width / 4:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Check if the output directory exists, create it if it doesn't\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "        if is_face_straight(file_path):\n",
    "            output_path = os.path.join(output_directory, file)\n",
    "            os.rename(file_path, output_path)\n",
    "        else:\n",
    "            os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE DUPLICATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "input_directory = r'path\\to\\your\\input\\directory'\n",
    "\n",
    "# Create a dictionary to store image hashes\n",
    "hashes = {}\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Open the image file\n",
    "            with Image.open(file_path) as img:\n",
    "                # Compute the hash of the image and convert it to a string\n",
    "                image_hash = str(imagehash.dhash(img))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # If the image hash is already in the dictionary, it's a duplicate\n",
    "        if image_hash in hashes:\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            hashes[image_hash] = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE NON-PEOPLE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Initialize the HOG descriptor\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Check if the output directory exists, create it if it doesn't\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "        image = cv2.imread(file_path)\n",
    "        boxes, weights = hog.detectMultiScale(image, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "        # If no humans are detected, move the image to the output directory\n",
    "        if len(boxes) == 0:\n",
    "            output_path = os.path.join(output_directory, file)\n",
    "            os.rename(file_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROP IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "output_directory = \"/Users/lethanhtien/Desktop/UIT/MACHINE LEARNING/CS114.O11-FinalProject/images/angry/output\"\n",
    "input_directory = \"/Users/lethanhtien/Desktop/UIT/MACHINE LEARNING/CS114.O11-FinalProject/images/angry\"\n",
    "# Check if the output directory exists, create it if it doesn't\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "        image = cv2.imread(file_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Initialize the Haar cascade classifier for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Expand the face region by 5px\n",
    "            x = max(0, x - 5)\n",
    "            y = max(0, y - 5)\n",
    "            if x + w + 10 <= image.shape[1] and y + h + 10 <= image.shape[0]:\n",
    "                w += 10\n",
    "                h += 10\n",
    "\n",
    "            # Crop the face region\n",
    "            face = image[y:y+h, x:x+w]\n",
    "\n",
    "            # Resize the face to 48x48 pixels\n",
    "            resized_face = cv2.resize(face, (48, 48))\n",
    "\n",
    "            # Save the cropped face image\n",
    "            output_path = os.path.join(output_directory, file)\n",
    "            cv2.imwrite(output_path, resized_face)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def detect_and_resize_faces(input_folder, output_folder, target_size=(48, 48)):\n",
    "    # Đảm bảo thư mục xuất khẩu tồn tại\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Lặp qua tất cả các tệp trong thư mục đầu vào\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Đọc ảnh từ đường dẫn\n",
    "        img = cv2.imread(input_path)\n",
    "        # Chuyển đổi sang đen trắng để tăng hiệu suất\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Tạo một bộ lọc khuôn mặt\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        # Lặp qua tất cả các khuôn mặt phát hiện được\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Cắt và chuyển đổi kích thước khuôn mặt\n",
    "            face = img[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face, target_size)\n",
    "\n",
    "            # Lưu ảnh đã xử lý vào thư mục đầu ra\n",
    "            cv2.imwrite(output_path, face_resized)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Thay đổi các đường dẫn tương ứng của bạn\n",
    "    input_folder_path = \"/Users/lethanhtien/Desktop/UIT/MACHINE LEARNING/archive/sad\"\n",
    "    output_folder_path = \"/Users/lethanhtien/Desktop/UIT/MACHINE LEARNING/sadoutput\"\n",
    "\n",
    "    detect_and_resize_faces(input_folder_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLACK, WHITE & CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "input_directory = r'happy\\input'\n",
    "output_directory = r'happy\\output'\n",
    "\n",
    "# Define the labels\n",
    "labels = {\n",
    "    \"angry\": 0,\n",
    "    \"fear\": 1,\n",
    "    \"sad\": 2,\n",
    "    \"neutral\": 3,\n",
    "    \"happy\": 4\n",
    "}\n",
    "\n",
    "# Get the name of the parent directory of the input directory\n",
    "parent_dir_name = os.path.basename(os.path.dirname(input_directory))\n",
    "csv_output_file = os.path.join(output_directory, f\"{parent_dir_name}.csv\")\n",
    "\n",
    "def convert_to_black_and_white(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "# Check if the output directory exists, create it if it doesn't\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Create an empty DataFrame to store the pixel data and labels\n",
    "data = pd.DataFrame(columns=[\"emotion\", \"pixels\", \"Usage\"])\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "        # Convert image to black and white\n",
    "        black_and_white_image = convert_to_black_and_white(file_path)\n",
    "\n",
    "        # Save black and white image with the original file name\n",
    "        output_path = os.path.join(output_directory, file)\n",
    "        cv2.imwrite(output_path, black_and_white_image)\n",
    "\n",
    "        # Convert black and white image to pixel matrix\n",
    "        pixel_matrix = np.array(black_and_white_image)\n",
    "\n",
    "        # Flatten the pixel matrix\n",
    "        pixel_matrix = pixel_matrix.flatten()\n",
    "\n",
    "        # Get the label\n",
    "        label = labels[parent_dir_name]\n",
    "\n",
    "        # Get the index from the file name\n",
    "        index = int(re.search(r'\\d+', file).group())\n",
    "\n",
    "        # Determine the usage based on the index\n",
    "        usage = \"Test\" if 250 <= index <= 300 else \"Training\"\n",
    "\n",
    "        # Convert the flattened pixel matrix to a space-separated string\n",
    "        pixels_str = \" \".join(map(str, pixel_matrix))\n",
    "\n",
    "        # Append the data to the DataFrame\n",
    "        new_data = pd.DataFrame({\"emotion\": [label], \"pixels\": [pixels_str], \"Usage\": [usage]})\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "data.to_csv(csv_output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "input_directory = r'image'\n",
    "output_directory = r'image'\n",
    "\n",
    "# Define the labels\n",
    "labels = {\n",
    "    \"angry\": 0,\n",
    "    \"fear\": 1,\n",
    "    \"sad\": 2,\n",
    "    \"neutral\": 3,\n",
    "    \"happy\": 4\n",
    "}\n",
    "\n",
    "# Get the name of the parent directory of the input directory\n",
    "parent_dir_name = os.path.basename(os.path.dirname(input_directory))\n",
    "csv_output_file = os.path.join(output_directory, \"dataset.csv\")  # Updated file name\n",
    "\n",
    "# Check if the output directory exists, create it if it doesn't\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Create an empty DataFrame to store the pixel data and labels\n",
    "data = pd.DataFrame(columns=[\"emotion\", \"pixels\", \"Usage\"])\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "        # Read the image in grayscale\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize the image to 48x48 pixels\n",
    "        resized_image = cv2.resize(image, (48, 48))\n",
    "\n",
    "        # Flatten the pixel matrix to a single array\n",
    "        pixel_matrix = resized_image.flatten()\n",
    "\n",
    "        # Flatten the pixel matrix\n",
    "        pixel_matrix = pixel_matrix.flatten()\n",
    "\n",
    "        # Extract emotion label from the filename\n",
    "        for emotion, label in labels.items():\n",
    "            if emotion in file.lower():\n",
    "                emotion_label = label\n",
    "                break\n",
    "        else:\n",
    "            # If no matching emotion label is found, skip the file\n",
    "            continue\n",
    "\n",
    "        # Get the index from the file name\n",
    "        index = int(re.search(r'\\d+', file).group())\n",
    "\n",
    "        # Determine the usage based on the index\n",
    "        usage = \"Test\" if 250 <= index <= 300 else \"Training\"\n",
    "\n",
    "        # Convert the flattened pixel matrix to a space-separated string\n",
    "        pixels_str = \" \".join(map(str, pixel_matrix))\n",
    "\n",
    "        # Append the data to the DataFrame\n",
    "        new_data = pd.DataFrame({\"emotion\": [emotion_label], \"pixels\": [pixels_str], \"Usage\": [usage]})\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "data.to_csv(csv_output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n"
     ]
    }
   ],
   "source": [
    "numbers = input(\"Enter an array: \")\n",
    "numbers = numbers.split()  # Split the input string into a list of numbers\n",
    "count = len(numbers)\n",
    "print(count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
